{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H1N1 Flu Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "sns.set(style=\"white\", context=\"notebook\", palette=\"deep\")\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Data\n",
    "train_X = pd.read_csv(r'...\\training_set_features.csv')\n",
    "train_Y = pd.read_csv(r'...\\training_set_labels.csv')\n",
    "test_X = pd.read_csv(r'...\\test_set_features.csv')\n",
    "\n",
    "dataset = pd.concat((train_X, test_X), axis = 0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we need to clean the data and remove all the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "respondent_id                      0\n",
       "h1n1_concern                     177\n",
       "h1n1_knowledge                   238\n",
       "behavioral_antiviral_meds        150\n",
       "behavioral_avoidance             421\n",
       "behavioral_face_mask              38\n",
       "behavioral_wash_hands             82\n",
       "behavioral_large_gatherings      159\n",
       "behavioral_outside_home          164\n",
       "behavioral_touch_face            256\n",
       "doctor_recc_h1n1                4320\n",
       "doctor_recc_seasonal            4320\n",
       "chronic_med_condition           1903\n",
       "child_under_6_months            1633\n",
       "health_worker                   1593\n",
       "health_insurance               24502\n",
       "opinion_h1n1_vacc_effective      789\n",
       "opinion_h1n1_risk                768\n",
       "opinion_h1n1_sick_from_vacc      770\n",
       "opinion_seas_vacc_effective      914\n",
       "opinion_seas_risk               1013\n",
       "opinion_seas_sick_from_vacc     1058\n",
       "age_group                          0\n",
       "education                       2814\n",
       "race                               0\n",
       "sex                                0\n",
       "income_poverty                  8920\n",
       "marital_status                  2850\n",
       "rent_or_own                     4078\n",
       "employment_status               2934\n",
       "hhs_geo_region                     0\n",
       "census_msa                         0\n",
       "household_adults                 474\n",
       "household_children               474\n",
       "employment_industry            26605\n",
       "employment_occupation          26896\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we convert the Categorical Variables into groups of Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, columns=['age_group',\n",
    "                                           'education',\n",
    "                                           'race',\n",
    "                                           'sex',\n",
    "                                           'income_poverty',\n",
    "                                           'marital_status',\n",
    "                                           'rent_or_own',\n",
    "                                           'employment_status',\n",
    "                                           'employment_industry',\n",
    "                                           'employment_occupation',\n",
    "                                           'hhs_geo_region',\n",
    "                                           'census_msa',\n",
    "                                           'health_insurance'],dummy_na=True)\n",
    "dataset = dataset.drop(['respondent_id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### None of the continuous variables have more than 5% null values. Thus imputing with the statistical mode will not change the distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "dataset = pd.DataFrame(mf_imputer.fit_transform(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  109  110  111  112  \\\n",
       "0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  ...  1.0  0.0  0.0  0.0   \n",
       "1  3.0  2.0  0.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0  ...  0.0  0.0  0.0  1.0   \n",
       "2  1.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  1.0  0.0  1.0   \n",
       "3  1.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "4  2.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0  1.0  0.0  ...  0.0  1.0  0.0  1.0   \n",
       "\n",
       "   113  114  115  116  117  118  \n",
       "0  0.0  1.0  0.0  0.0  1.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "3  1.0  0.0  0.0  0.0  0.0  1.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "\n",
       "[5 rows x 119 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can reduce the dimensionality by applying PCA over the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05465351 0.09414192 0.12807185 0.16017798 0.18331442 0.20374225\n",
      " 0.22328407 0.2422337  0.25982166 0.27632395 0.29222597 0.30792893\n",
      " 0.32300316 0.3375437  0.35157603 0.36550629 0.37922303 0.39268312\n",
      " 0.40565064 0.41840927 0.43101092 0.44357903 0.45578206 0.46767296\n",
      " 0.47941967 0.49109893 0.50268177 0.5139817  0.52520192 0.53627986\n",
      " 0.54724308 0.55803837 0.56861704 0.57898631 0.58920534 0.59928817\n",
      " 0.609234   0.61898105 0.62867256 0.63827606 0.6477823  0.65719543\n",
      " 0.66654443 0.67583922 0.68496408 0.69397371 0.70297955 0.71186518\n",
      " 0.72072551 0.72951992 0.73829713 0.74697341 0.75561167 0.76421213\n",
      " 0.77269002 0.78107353 0.78942957 0.79753646 0.80549505 0.81341088\n",
      " 0.82117843 0.828882   0.83639265 0.84372596 0.85094066 0.85813176\n",
      " 0.86522977 0.87213572 0.87878126 0.88523624 0.89152453 0.89763404\n",
      " 0.90372361 0.90947155 0.91514905 0.92060775 0.9260516  0.93131944\n",
      " 0.93651532 0.94157989 0.94637283 0.95113068 0.95542948 0.95964374\n",
      " 0.96344348 0.96716896 0.97077485 0.97432847 0.97781335 0.98102197\n",
      " 0.98403307 0.98698792 0.9898651  0.99253416 0.99496501 0.9970337\n",
      " 0.99831023 0.99938401 0.99978714 0.99998563 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = 118)\n",
    "pca.fit(dataset)\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We aim to keep 98% of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_fin = PCA(n_components=90)\n",
    "dataset = pca_fin.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.477269</td>\n",
       "      <td>-0.939399</td>\n",
       "      <td>-2.810101</td>\n",
       "      <td>1.677450</td>\n",
       "      <td>0.960905</td>\n",
       "      <td>-0.907742</td>\n",
       "      <td>-0.122672</td>\n",
       "      <td>-1.947278</td>\n",
       "      <td>0.287389</td>\n",
       "      <td>-1.336138</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.126500</td>\n",
       "      <td>0.082598</td>\n",
       "      <td>0.109705</td>\n",
       "      <td>-0.122186</td>\n",
       "      <td>-0.062488</td>\n",
       "      <td>-0.842833</td>\n",
       "      <td>-1.009889</td>\n",
       "      <td>-0.677297</td>\n",
       "      <td>0.411287</td>\n",
       "      <td>-0.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.099628</td>\n",
       "      <td>-0.248041</td>\n",
       "      <td>1.754716</td>\n",
       "      <td>2.819024</td>\n",
       "      <td>-0.476688</td>\n",
       "      <td>-0.533966</td>\n",
       "      <td>-1.517075</td>\n",
       "      <td>1.717396</td>\n",
       "      <td>0.083797</td>\n",
       "      <td>2.123076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.948365</td>\n",
       "      <td>0.338174</td>\n",
       "      <td>-0.107095</td>\n",
       "      <td>-0.371576</td>\n",
       "      <td>-0.091988</td>\n",
       "      <td>-1.310901</td>\n",
       "      <td>-0.907987</td>\n",
       "      <td>-0.857585</td>\n",
       "      <td>-0.251214</td>\n",
       "      <td>-0.124386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.151864</td>\n",
       "      <td>1.753960</td>\n",
       "      <td>-2.422692</td>\n",
       "      <td>1.855613</td>\n",
       "      <td>0.260078</td>\n",
       "      <td>-0.102172</td>\n",
       "      <td>-0.520670</td>\n",
       "      <td>0.288324</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-0.491008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367216</td>\n",
       "      <td>-0.129235</td>\n",
       "      <td>0.259698</td>\n",
       "      <td>0.339762</td>\n",
       "      <td>0.640877</td>\n",
       "      <td>-0.641961</td>\n",
       "      <td>0.007404</td>\n",
       "      <td>-0.071285</td>\n",
       "      <td>-0.504422</td>\n",
       "      <td>-0.059059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.564117</td>\n",
       "      <td>-2.121177</td>\n",
       "      <td>0.111360</td>\n",
       "      <td>1.503419</td>\n",
       "      <td>1.101802</td>\n",
       "      <td>0.880085</td>\n",
       "      <td>-0.830469</td>\n",
       "      <td>0.791518</td>\n",
       "      <td>-0.262625</td>\n",
       "      <td>-0.586774</td>\n",
       "      <td>...</td>\n",
       "      <td>1.021343</td>\n",
       "      <td>-0.184607</td>\n",
       "      <td>-1.845998</td>\n",
       "      <td>-0.350741</td>\n",
       "      <td>0.215428</td>\n",
       "      <td>0.849324</td>\n",
       "      <td>1.044665</td>\n",
       "      <td>0.488705</td>\n",
       "      <td>0.803333</td>\n",
       "      <td>0.131655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.207815</td>\n",
       "      <td>0.128450</td>\n",
       "      <td>0.547580</td>\n",
       "      <td>-0.965854</td>\n",
       "      <td>-0.269713</td>\n",
       "      <td>-0.799702</td>\n",
       "      <td>-3.623682</td>\n",
       "      <td>-2.019973</td>\n",
       "      <td>2.790949</td>\n",
       "      <td>-0.874182</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.628074</td>\n",
       "      <td>0.439784</td>\n",
       "      <td>0.549139</td>\n",
       "      <td>-0.047243</td>\n",
       "      <td>0.257834</td>\n",
       "      <td>-0.336099</td>\n",
       "      <td>1.493621</td>\n",
       "      <td>0.610300</td>\n",
       "      <td>-0.997458</td>\n",
       "      <td>-0.236407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  2.477269 -0.939399 -2.810101  1.677450  0.960905 -0.907742 -0.122672   \n",
       "1 -1.099628 -0.248041  1.754716  2.819024 -0.476688 -0.533966 -1.517075   \n",
       "2 -2.151864  1.753960 -2.422692  1.855613  0.260078 -0.102172 -0.520670   \n",
       "3  3.564117 -2.121177  0.111360  1.503419  1.101802  0.880085 -0.830469   \n",
       "4 -2.207815  0.128450  0.547580 -0.965854 -0.269713 -0.799702 -3.623682   \n",
       "\n",
       "         7         8         9   ...        80        81        82        83  \\\n",
       "0 -1.947278  0.287389 -1.336138  ... -0.126500  0.082598  0.109705 -0.122186   \n",
       "1  1.717396  0.083797  2.123076  ... -0.948365  0.338174 -0.107095 -0.371576   \n",
       "2  0.288324 -0.006329 -0.491008  ...  0.367216 -0.129235  0.259698  0.339762   \n",
       "3  0.791518 -0.262625 -0.586774  ...  1.021343 -0.184607 -1.845998 -0.350741   \n",
       "4 -2.019973  2.790949 -0.874182  ... -0.628074  0.439784  0.549139 -0.047243   \n",
       "\n",
       "         84        85        86        87        88        89  \n",
       "0 -0.062488 -0.842833 -1.009889 -0.677297  0.411287 -0.074000  \n",
       "1 -0.091988 -1.310901 -0.907987 -0.857585 -0.251214 -0.124386  \n",
       "2  0.640877 -0.641961  0.007404 -0.071285 -0.504422 -0.059059  \n",
       "3  0.215428  0.849324  1.044665  0.488705  0.803333  0.131655  \n",
       "4  0.257834 -0.336099  1.493621  0.610300 -0.997458 -0.236407  \n",
       "\n",
       "[5 rows x 90 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = dataset.iloc[:26707,:]\n",
    "test_X = dataset.iloc[26707:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We should now split the dataset into training and cross validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_split(train_X, train_Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifiers have been used as a model. For a better fit we can increase the number of trees - \"n_estimators\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1n1_class = RandomForestClassifier(n_estimators = 100, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=2,\n",
       "                       oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1n1_class.fit(X_train, y_train['h1n1_vaccine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_class = RandomForestClassifier(n_estimators = 100, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=2,\n",
       "                       oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seasonal_class.fit(X_train, y_train['seasonal_vaccine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_h1n1 = h1n1_class.predict(X_cv)\n",
    "y_pred_seasonal = seasonal_class.predict(X_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the accuracy of the models can be checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8315237738674653\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_cv[\"h1n1_vaccine\"], y_pred_h1n1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7461624859603145\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_cv[\"seasonal_vaccine\"], y_pred_seasonal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The accuracies arent in the 90s as I would like them to be. Improvements can be made in Feature Engineering and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
